{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hello Team, I think this is your first encounter with neural networks. This mini-project is to help you understand the various steps of learning with a neural network. This time, together you'll create a simple feed-forward neural net. Complexity of the neural nets that you'll create will keep increasing with time.\n",
    "\n",
    "#### The learning problem: We'll try to create an AI that can recognize handwritten digits in the famous MNIST database (the same one 3Blue1Brown's video series on neural networks is based on, if you haven't watched at least the first two videos in the series, I can guarantee you that you won't find a more clear/visual explanation on the web).\n",
    "\n",
    "#### How it's going to work: Below I have laid out the different steps in the process. Every member's task is to write the part of the code that he/she has been assigned to. If your understanding of how an ANN works is clear, hopefully all your contributions will wake up our AI (even if you don't communicate with each other!). \n",
    "\n",
    "#### We'll be using Keras to implement the neural network as I've said before. Steps are of course sequential. Good luck!\n",
    "\n",
    "# ref: https://medium.com/tebs-lab/how-to-classify-mnist-digits-with-different-neural-network-architectures-39c75a0f03e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the dataset \n",
    "import numpy as np\n",
    "from keras.datasets import mnist# <--- My contribution to our little AI\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 1: Create the data pipeline to feed the network (To be done by member 1)\n",
    "\"\"\"\n",
    "You have an image (or loads of them) which will give you a numeric feature vector that \n",
    "can be fed to the input layer of the neural net. But is that all the network is hungry for? No.\n",
    "It also requires a label for the image, in this case 0 or 1 or ... or 9. But the output layer of a \n",
    "neural net is often a probability distribution (meaning the sum of all the node-values or the \n",
    "activations of the output layer is 1) and the number of nodes in the output layer = number of \n",
    "different possible labels or class which for this case is 10. This means that you can interpret the \n",
    "activation of the j-th node in the output layer as the probability that the input belongs to the \n",
    "j-th class or has the j-th label. If you think about the cost function, you'll understand that we \n",
    "should convert our rudimentary labels into probability vectors too. So an image depicting 2 should \n",
    "have the label (0,0,1,0,...,0).\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Your mission should you choose to accept it, is to:\n",
    "1. Create a Python object called digits having two attributes train, test\n",
    "2. Make sure digit.train and digit.test are objects of the same type having two \n",
    "attributes features(=array of all the feature vectors) and labels(=array of all the label vectors).\n",
    "Every element of features is a numeric array representing an image \n",
    "(might be an array of greyscale intensity, might be something else).\n",
    "\"\"\"\n",
    "class Dataset():\n",
    "    def __init__(self,train,test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "class Data():\n",
    "    def __init__(self,features,labels):\n",
    "        self.features = features.astype('float32')/ 255.0\n",
    "        new_labels=np.zeros([labels.shape[0],10])\n",
    "        for i,label in enumerate(labels):\n",
    "            new_labels[i,label]=1\n",
    "        self.labels= new_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train=Data(trainX,trainy)\n",
    "test=Data(testX,testy)\n",
    "digits=Dataset(train,test)\n",
    "# to import train features: digits.train.features\n",
    "# to import train features: digits.train.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/40\n",
      "54000/54000 [==============================] - 1s 21us/step - loss: 2.4196 - accuracy: 0.1401 - val_loss: 2.2897 - val_accuracy: 0.1795\n",
      "Epoch 2/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 2.2474 - accuracy: 0.2214 - val_loss: 2.1937 - val_accuracy: 0.3247\n",
      "Epoch 3/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 2.1713 - accuracy: 0.4222 - val_loss: 2.1276 - val_accuracy: 0.5407\n",
      "Epoch 4/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 2.1109 - accuracy: 0.5485 - val_loss: 2.0676 - val_accuracy: 0.6137\n",
      "Epoch 5/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 2.0542 - accuracy: 0.6008 - val_loss: 2.0096 - val_accuracy: 0.6515\n",
      "Epoch 6/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.9990 - accuracy: 0.6324 - val_loss: 1.9527 - val_accuracy: 0.6693\n",
      "Epoch 7/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.9448 - accuracy: 0.6485 - val_loss: 1.8967 - val_accuracy: 0.6888\n",
      "Epoch 8/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.8916 - accuracy: 0.6639 - val_loss: 1.8418 - val_accuracy: 0.7032\n",
      "Epoch 9/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.8395 - accuracy: 0.6764 - val_loss: 1.7880 - val_accuracy: 0.7142\n",
      "Epoch 10/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.7883 - accuracy: 0.6857 - val_loss: 1.7354 - val_accuracy: 0.7215\n",
      "Epoch 11/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.7384 - accuracy: 0.6926 - val_loss: 1.6839 - val_accuracy: 0.7317\n",
      "Epoch 12/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.6896 - accuracy: 0.7027 - val_loss: 1.6338 - val_accuracy: 0.7370\n",
      "Epoch 13/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.6422 - accuracy: 0.7093 - val_loss: 1.5852 - val_accuracy: 0.7428\n",
      "Epoch 14/40\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 1.5963 - accuracy: 0.7151 - val_loss: 1.5380 - val_accuracy: 0.7518\n",
      "Epoch 15/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.5518 - accuracy: 0.7221 - val_loss: 1.4925 - val_accuracy: 0.7588\n",
      "Epoch 16/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 1.5089 - accuracy: 0.7282 - val_loss: 1.4486 - val_accuracy: 0.7657\n",
      "Epoch 17/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 1.4677 - accuracy: 0.7333 - val_loss: 1.4065 - val_accuracy: 0.7703\n",
      "Epoch 18/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 1.4281 - accuracy: 0.7401 - val_loss: 1.3661 - val_accuracy: 0.7768\n",
      "Epoch 19/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.3902 - accuracy: 0.7454 - val_loss: 1.3274 - val_accuracy: 0.7842\n",
      "Epoch 20/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.3540 - accuracy: 0.7515 - val_loss: 1.2904 - val_accuracy: 0.7888\n",
      "Epoch 21/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.3194 - accuracy: 0.7557 - val_loss: 1.2552 - val_accuracy: 0.7953\n",
      "Epoch 22/40\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 1.2863 - accuracy: 0.7597 - val_loss: 1.2215 - val_accuracy: 0.8003\n",
      "Epoch 23/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 1.2549 - accuracy: 0.7656 - val_loss: 1.1895 - val_accuracy: 0.8040\n",
      "Epoch 24/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.2249 - accuracy: 0.7703 - val_loss: 1.1589 - val_accuracy: 0.8085\n",
      "Epoch 25/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.1963 - accuracy: 0.7730 - val_loss: 1.1298 - val_accuracy: 0.8150\n",
      "Epoch 26/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.1691 - accuracy: 0.7771 - val_loss: 1.1021 - val_accuracy: 0.8190\n",
      "Epoch 27/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.1432 - accuracy: 0.7811 - val_loss: 1.0757 - val_accuracy: 0.8237\n",
      "Epoch 28/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.1186 - accuracy: 0.7847 - val_loss: 1.0506 - val_accuracy: 0.8272\n",
      "Epoch 29/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.0951 - accuracy: 0.7889 - val_loss: 1.0266 - val_accuracy: 0.8303\n",
      "Epoch 30/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.0726 - accuracy: 0.7918 - val_loss: 1.0038 - val_accuracy: 0.8335\n",
      "Epoch 31/40\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 1.0513 - accuracy: 0.7952 - val_loss: 0.9820 - val_accuracy: 0.8358\n",
      "Epoch 32/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 1.0309 - accuracy: 0.7979 - val_loss: 0.9612 - val_accuracy: 0.8398\n",
      "Epoch 33/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 1.0115 - accuracy: 0.8011 - val_loss: 0.9414 - val_accuracy: 0.8423\n",
      "Epoch 34/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.9929 - accuracy: 0.8034 - val_loss: 0.9224 - val_accuracy: 0.8470\n",
      "Epoch 35/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.9752 - accuracy: 0.8062 - val_loss: 0.9043 - val_accuracy: 0.8500\n",
      "Epoch 36/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.9582 - accuracy: 0.8089 - val_loss: 0.8870 - val_accuracy: 0.8518\n",
      "Epoch 37/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.9420 - accuracy: 0.8111 - val_loss: 0.8705 - val_accuracy: 0.8535\n",
      "Epoch 38/40\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.9265 - accuracy: 0.8136 - val_loss: 0.8546 - val_accuracy: 0.8548\n",
      "Epoch 39/40\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.9116 - accuracy: 0.8155 - val_loss: 0.8395 - val_accuracy: 0.8562\n",
      "Epoch 40/40\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.8973 - accuracy: 0.8180 - val_loss: 0.8249 - val_accuracy: 0.8580\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Activation\n",
    "# Step - 2: Design the network (To be done by member 2)\n",
    "\"\"\"\n",
    "The feature and label vectors have already determined the number of nodes in the input, output layers.\n",
    "But so many things are still unknown! Like the number of hidden layers, the number of nodes in each \n",
    "hidden layer, the activation functions.... Your duty might look heavy but I assure you it's only a \n",
    "few lines of code.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The set of codes here achieve the following tasks:\n",
    "1. Create a Sequential object and name it model.\n",
    "2. Add input, hidden and output layers to it with proper activation functions.\n",
    "3. Make sure the activation function for the output layer is softmax, otherwise \n",
    "we won't get a probability vector as the output! \n",
    "4. Compile the model i.e. choose a loss function and optimizer for our neural net.\n",
    "5. Learn from the training samples using model.fit with a proper number of epochs \n",
    "and batch_size and store the result in an object named learning_history.\n",
    "\"\"\"\n",
    "\n",
    "#Create a model by instantiating an object of the class sequential\n",
    "model=Sequential()\n",
    "\n",
    "#We need to add layers to our model by specifying no. of neurons and activation \n",
    "#function along with other parameters set to their default values\n",
    "\n",
    "\"\"\"A dense layer is a type of layer where each neuron is connected to \n",
    "every other neuron in the previous layer\"\"\"\n",
    "#The first layer\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "\n",
    "#The Hidden Layers:relu is rectified linear(look up wikipedia)\n",
    "model.add(Dense(64,activation='sigmoid', input_shape=(28*28,)))\n",
    "#model.add(Dense(256,activation='relu'))    \n",
    "\n",
    "#Output layer must have 'softmax' activation(look up wikipedia)as we want probabilities in the output layer.\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "#The Last step is to compile the model by specifying the cost-function \n",
    "#and the optimization method used to learn the parameters(weights of the NN)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "\n",
    "#Now train the data on the training Data Set and evlaute on test Data set\n",
    "learning_history=model.fit(digits.train.features,train.labels,batch_size=1000,epochs=40,shuffle=True,validation_split=.1)\n",
    "\n",
    "# Saving the model\n",
    "#model.save('knowledge.h5')\n",
    "\n",
    "# If one wants to delete the current model in memory\n",
    "#del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving the model\n",
    "model.save('knowledge.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 43us/step\n",
      "60000/60000 [==============================] - 2s 38us/step\n",
      "Train score:0.8837  Test Score: 0.8641 \n",
      "Train Accuracy: 82.31 Test Accuracy: 83.09\n"
     ]
    }
   ],
   "source": [
    "# Step - 3: Save/Load and Test our AI (To be done by member 3)\n",
    "# Loading the model\n",
    "from keras.models import load_model\n",
    "our_1st_ai=load_model('knowledge.h5')\n",
    "\n",
    "\"\"\"\n",
    "Last but not the least:\n",
    "1. Save our model as knowledge.h5 file in the same folder as this notebook for future use.\n",
    "2. Load the neural network saved in knowledge.h5 into an object named our_1st_ai.\n",
    "3. Output the accuracy score of our_1st_ai on the training and test samples separately.\n",
    "4. Plot the loss function and model accuracy as a function of epochs using learning_history. \n",
    "\"\"\"\n",
    "\n",
    "score_test,acc_test=model.evaluate(digits.test.features,digits.test.labels,batch_size=100)\n",
    "score_train,acc_train=model.evaluate(digits.train.features,digits.train.labels,batch_size=100)\n",
    "print('Train score:%.4f '%score_train,'Test Score: %.4f '%score_test)\n",
    "print('Train Accuracy: %.2f'%(acc_train*100),'Test Accuracy: %.2f'%(acc_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(model.history.history['accuracy'])\n",
    "plt.plot(model.history.history['val_accuracy'])\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.axhline(1,linestyle='dashed')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation','loss'], loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7fa8d001b978>,\n",
       "  <matplotlib.lines.Line2D at 0x7fa8d001bcc0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7fa8d001bda0>,\n",
       "  <matplotlib.lines.Line2D at 0x7fa8d0028390>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7fa8d001b550>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7fa8d00286d8>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7fa8d0028a20>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(model.history.history['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srashti/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for a an image.\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "    # load the image\n",
    "    img = load_img(filename, grayscale=True, target_size=(28, 28))\n",
    "    # convert to array\n",
    "    img = img_to_array(img)\n",
    "    # reshape into a single sample with 1 channel\n",
    "    img = img.reshape( 1,28, 28)\n",
    "    # prepare pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "    # load the image\n",
    "    img = load_image('sample_image.png')\n",
    "    # load model\n",
    "    model = load_model('knowledge.h5')\n",
    "    # predict the class\n",
    "    digit = model.predict_classes(img)\n",
    "    print(digit[0])\n",
    "# entry point, run the example\n",
    "run_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
